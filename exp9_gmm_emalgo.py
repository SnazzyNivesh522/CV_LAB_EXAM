# -*- coding: utf-8 -*-
"""Exp9_GMM_EMAlgo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OV_lBLUwaDvDonCD2BqZpjDINvjW5weL
"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture as GMM

image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/CV/cameraman.tif', cv2.IMREAD_GRAYSCALE)
plt.imshow(image, cmap='gray')
plt.title('Cameraman Image')
plt.axis('off')
plt.show()

gmm_model=GMM(n_components=2,random_state=0)
gmm_model.fit(image.reshape(-1,1))
gmm_labels=gmm_model.predict(image.reshape(-1,1))
gmm_labels=gmm_labels.reshape(image.shape)
plt.imshow(gmm_labels,cmap='gray')
plt.title('GMM Segmentation 2 CLUSTERS')
plt.axis('off')
plt.show()

gmm_model=GMM(n_components=3,random_state=0)
gmm_model.fit(image.reshape(-1,1))
gmm_labels=gmm_model.predict(image.reshape(-1,1))
gmm_labels=gmm_labels.reshape(image.shape)
plt.imshow(gmm_labels,cmap='gray')
plt.title('GMM Segmentation CLUSTERS 3')
plt.axis('off')
plt.show()

gmm_model.means_

gmm_model.covariances_

gmm_model.weights_

gmm_model.lower_bound_

# prompt: plot the graph of log likelihood of GMM Model

import matplotlib.pyplot as plt

n_components = np.arange(1, 21)
models = [GMM(n, random_state=0).fit(image.reshape(-1, 1))
          for n in n_components]

plt.plot(n_components, [m.lower_bound_ for m in models], label="Log-Likelihood")
plt.xlabel("Number of components")
plt.ylabel("Log-Likelihood")
plt.title("GMM Log-Likelihood vs Number of components")
plt.legend(loc='best')
plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt

def multivariate_gdf(x,mean_vector,covariance_matrix):
  """Calculate the multivariate Gaussian PDF."""
  D=len(mean_vector)
  if covariance_matrix.ndim == 0:
        covariance_matrix = covariance_matrix.reshape(1, 1)
  covariance_matrix_inv=np.linalg.inv(covariance_matrix)
  covariance_matrix_det=np.linalg.det(covariance_matrix)
  x_uk=(x-mean_vector).reshape(-1,1)

  pdf=np.exp(-0.5 * x_uk.T @ covariance_matrix_inv @ x_uk) / np.sqrt((2*np.pi)**D * covariance_matrix_det)
  return pdf


def expectation_step(data,means,covariances,mixing_coeffs,n_components):
  n_samples=data.shape[0]
  responsibilities=np.zeros((n_samples,n_components))
  for k in range(n_components):
    for i in range(n_samples):
      responsibilities[i,k]=mixing_coeffs[k]*multivariate_gdf(data[i],means[k],covariances[k])
  responsibilities/=np.sum(responsibilities,axis=1)[:,np.newaxis]
  return responsibilities


def maximization_step(data,responsibilities,n_components):
  n_samples,n_features=data.shape
  means=np.zeros((n_components,n_features))
  covariances=[]
  mixing_coeffs=np.zeros(n_components)
  for k in range(n_components):
    responsibility = responsibilities[:, [k]]
    total_responsibility = responsibility.sum()
    mixing_coeffs[k] = total_responsibility / n_samples
    means[k] = (responsibility * data).sum(axis=0) / total_responsibility
    cov=np.zeros((n_features,n_features))
    for j in range(n_samples):
      x_uk=(data[j]-means[k]).reshape(-1,1)
      cov+=responsibility[j] * (x_uk @ x_uk.T)
    cov/=total_responsibility
    covariances.append(cov)
  return means,covariances,mixing_coeffs


def compute_log_likelihood(data,means,covariances,mixing_coeffs,n_components):
  log_likelihood=0
  n_samples=data.shape[0]
  for i in range(n_samples):
    sample=data[i]
    likelihood=0
    for k in range(n_components):
      likelihood+=mixing_coeffs[k]*multivariate_gdf(sample,means[k],covariances[k])
    log_likelihood+=np.log(likelihood)
  return log_likelihood


def gaussian_mixture_model(data,n_components,max_iter=1000,threshold=1e-6):
  n_samples=data.shape[0]
  #Initialization Step
  indices=np.random.choice(n_samples,n_components,replace=False)

  means=data[indices]
  covariances=[np.cov(data.T) for _ in range(n_components)]
  mixing_coeffs=np.ones(n_components)/n_components

  log_likelihoods=[]
  responsibilities=[[]]
  #iterate until convergence is reached
  for i in range(max_iter):
    #E-step
    responsibilities=expectation_step(data,means,covariances,mixing_coeffs,n_components)

    #M-step
    means,covariances,mixing_coeffs=maximization_step(data,responsibilities,n_components)

    #Log likelihood calculation
    log_likelihood=compute_log_likelihood(data,means,covariances,mixing_coeffs,n_components)
    log_likelihoods.append(log_likelihood)
    print(f"Iteration : {i}")
    print(f"Means : {means}")
    print(f"Covariances : {covariances}")
    print(f"Mixing Coefficients : {mixing_coeffs}")
    print(f"Log Likelihood : {log_likelihood}")
    print(f"Responsibilities : {responsibilities}")
    if i> 0 and abs(log_likelihood - log_likelihoods[-2]) < threshold:
      print(f"Convergence reached at iteration {i}")
      break

  return means,covariances,mixing_coeffs,responsibilities,log_likelihoods

image=cv2.imread('/content/drive/MyDrive/Colab Notebooks/CV/cameraman.tif',cv2.IMREAD_GRAYSCALE)



means,covariances,mixing_coeffs,responsibilities,log_likelihoods=gaussian_mixture_model(image.reshape(-1,1),n_components=3,max_iter=100,threshold=1e-3)

labels = np.argmax(responsibilities, axis=1)
segmented_image = labels.reshape(image.shape)

# Display the Original and Segmented Image
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title("Segmented Image")
plt.imshow(segmented_image, cmap='gray')
plt.axis('off')
plt.show()

# Task 5: Plot the Log-Likelihood Convergence
plt.figure(figsize=(8, 4))
plt.title("Log-Likelihood Convergence")
plt.plot(np.array(log_likelihoods).flatten())
plt.xlabel("Iteration")
plt.ylabel("Log-Likelihood")
plt.grid()
plt.show()
